{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidKrugerT/scalable_lab2/blob/main/Kopia_av_wisperTrainingpipline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ3IPpJWoZf2"
      },
      "source": [
        "# Connecting to drive and loading in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlxVfdIK8Sop"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzIEEmuspYAv"
      },
      "outputs": [],
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6XbVF7TIyI2"
      },
      "outputs": [],
      "source": [
        "pip install ray[tune]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p1bwGsTKxmE"
      },
      "outputs": [],
      "source": [
        "from ray import tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6wxDIlhp5dc"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cagr9axVoSPk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL8XUKndpKhs"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "dataset = DatasetDict.load_from_disk(\"/content/gdrive/My Drive/whisper/features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT1q_KtEqSXH"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEpVESRO5Tlp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C03Y2yt5UnD"
      },
      "source": [
        "# Define Data collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFvfubGg5SkT"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperProcessor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYI9qyWy55Rd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwr5evBd567E"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOfu-kjg6X--"
      },
      "source": [
        "#Evaluation Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2jpW1II6XsT"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXZW1veO6vuD"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQGmYRIb60GY"
      },
      "source": [
        "#Load pretraind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtH2f1wv63BB"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ExKdsdB6-tC"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ_HFctHC-a1"
      },
      "source": [
        "#Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuhrvByaCOlh"
      },
      "outputs": [],
      "source": [
        "def sigopt_hp_space(trial):\n",
        "    return [\n",
        "        {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n",
        "        {\n",
        "            \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n",
        "            \"name\": \"per_device_train_batch_size\",\n",
        "            \"type\": \"categorical\",\n",
        "        },\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DUkusZbIJ7L_"
      },
      "outputs": [],
      "source": [
        "#Ray\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYZs7YvwJ9h0"
      },
      "outputs": [],
      "source": [
        "def ray_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n",
        "        \"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvhFqaeIDvYe"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "def model_init(trial):\n",
        "    return WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", return_dict = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSvRUGokGD7V"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    num_train_epochs=1,\n",
        "    output_dir=\"./content/gdrive/My Drive/whisper/models/hyperparameter\",  # change to a repo name of your choice\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"checkpoint\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXbY7KvqEFBh"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    model_init=model_init,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JH65n4PTFd_t"
      },
      "outputs": [],
      "source": [
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"minimize\",\n",
        "    backend=\"ray\",\n",
        "    hp_space=ray_hp_space,\n",
        "    n_trials=20,\n",
        "    compute_objective=compute_metrics,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rvvt1nldFvCc"
      },
      "outputs": [],
      "source": [
        "print(best_trial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "570-yuBk7CDa"
      },
      "source": [
        "#Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dpRHOw9P7RsH"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    num_train_epochs=1,\n",
        "    output_dir=\"./content/gdrive/My Drive/whisper/models\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"checkpoint\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kIN0Quof7XU6"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xwbd_dYq7jlB"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInRWK23Aeut"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ACJCii76AhtA"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Twjz3-4RCEGA"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"se\",\n",
        "    \"model_name\": \"Whisper Small Se - Swedish\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8WXNkAFLCKyQ"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}